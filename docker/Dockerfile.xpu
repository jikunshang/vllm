# oneapi 2025.0.2 docker base image use rolling 2448 package. https://dgpu-docs.intel.com/releases/packages.html?release=Rolling+2448.13&os=Ubuntu+22.04, and we don't need install driver manually.
FROM intel/deep-learning-essentials:2025.0.2-0-client-driver-libze1.20.2.0-devel-ubuntu24.10 AS vllm-base
RUN apt-get update -y && \
    apt-get install -y --no-install-recommends --fix-missing \
    curl \
    ffmpeg \
    git \
    libsndfile1 \
    libsm6 \
    libxext6 \
    libgl1 \
    lsb-release \
    numactl \
    wget

WORKDIR /workspace/vllm
COPY requirements/xpu.txt /workspace/vllm/requirements/xpu.txt
COPY requirements/common.txt /workspace/vllm/requirements/common.txt

ENV PATH="/miniforge3/bin:$PATH"

RUN --mount=type=cache,target=/miniforge3/pkgs \
    conda run --no-capture-output -n py310 \
    pip install --no-cache-dir \
    -r requirements/xpu.txt

ENV LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/lib/"

COPY . .
ARG GIT_REPO_CHECK=0
RUN --mount=type=bind,source=.git,target=.git \
    if [ "$GIT_REPO_CHECK" != 0 ]; then bash tools/check_repo.sh; fi

ENV VLLM_TARGET_DEVICE=xpu
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn

RUN --mount=type=cache,target=/miniforge3/pkgs \
    --mount=type=bind,source=.git,target=.git \
    conda run --no-capture-output -n py310 \
    python3 setup.py install

RUN echo "cd /workspace/vllm" >> ~/.bashrc
CMD ["/bin/bash"]

FROM vllm-base AS vllm-openai

# install additional dependencies for openai api server
RUN --mount=type=cache,target=/miniforge3/pkgs \
    conda run --no-capture-output -n py310 \
    pip install accelerate hf_transfer 'modelscope!=1.15.0'

ENV VLLM_USAGE_SOURCE production-docker-image \
    TRITON_XPU_PROFILE 1
# install development dependencies (for testing)
RUN conda run --no-capture-output -n py310 \
    python3 -m pip install -e tests/vllm_test_utils
ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
